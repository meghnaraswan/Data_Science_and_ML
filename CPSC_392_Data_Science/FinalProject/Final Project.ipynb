{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project (100 points)\n",
    "\n",
    "PLEASE READ ALL THE DIRECTIONS BEFORE STARTING THE PROJECT.\n",
    "\n",
    "\n",
    "## Part I\n",
    "\n",
    "### **1.**\n",
    "Find a data set that's interesting to you. Make sure it has at least 7 variables and at least 100 rows, and at least 4 continuous/interval columns. But the more the better. You may **NOT** use any of the datasets we've used in class (see [here](https://github.com/cmparlettpelleriti/CPSC392ParlettPelleriti/tree/master/Data)).\n",
    "\n",
    "Some places to find data:\n",
    "\n",
    "<ul>\n",
    "    <li> data.gov\n",
    "    <li> kaggle.com/datasets\n",
    "    <li> your own data! (e.g. fitbit, data from a video game you play, etc...)\n",
    "    <li> https://github.com/BuzzFeedNews\n",
    "    <li> http://archive.ics.uci.edu/ml/index.php\n",
    "    <li> https://www.quandl.com/search\n",
    "    <li> http://academictorrents.com/browse.php\n",
    "    <li> your favorite sports teams!\n",
    "    <li> <a href=\"http://billpetti.github.io/baseballr/about/\">baseball data</a>\n",
    "    <li> <a href=\"https://www.rdocumentation.org/packages/spotifyr/versions/1.0.0\">spotify data</a>\n",
    "    <li> <a href = \"https://www.nutritionix.com/database\">Fast Food and Food Data </a>\n",
    "    <li> Data from your job/internship\n",
    "    <li> Scrape twitter data\n",
    "    <li> <a href=\"https://github.com/rfordatascience/tidytuesday/tree/master/data\">Tidy Tuesday Data</a>\n",
    "    <li> <a href=\"https://github.com/fivethirtyeight/data\">fivethirtyeight</a>\n",
    "    <li> <a href=\"https://developer.riotgames.com/\"> League of Legends</a>\n",
    "</ul> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python packages needed\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import spotipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "import sklearn as sk\n",
    "from local import config as local_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our connection to the API\n",
    "username = \"megraswan\" # username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect\n",
    "client_credentials_manager = SpotifyClientCredentials(local_conf.CLIENT_ID, local_conf.CLIENT_SECRET)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>track_number</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Every Breath You Take</td>\n",
       "      <td>253920</td>\n",
       "      <td>7</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.452</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.796</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0348</td>\n",
       "      <td>0.5430</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.740</td>\n",
       "      <td>117.401</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>1JSTJqkT5qHq8MDJnJbRE1</td>\n",
       "      <td>spotify:track:1JSTJqkT5qHq8MDJnJbRE1</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/1JSTJqkT5qHq...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/1JST...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Don't You (Forget About Me)</td>\n",
       "      <td>263040</td>\n",
       "      <td>1</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.816</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>0.1680</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.678</td>\n",
       "      <td>111.346</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>3fH4KjXFYMmljxrcGrbPj9</td>\n",
       "      <td>spotify:track:3fH4KjXFYMmljxrcGrbPj9</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/3fH4KjXFYMml...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/3fH4...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Take on Me</td>\n",
       "      <td>225280</td>\n",
       "      <td>1</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.902</td>\n",
       "      <td>6</td>\n",
       "      <td>-7.638</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0540</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>0.876</td>\n",
       "      <td>84.412</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>2WfaOiMkCvy7F5fcp2zZ8L</td>\n",
       "      <td>spotify:track:2WfaOiMkCvy7F5fcp2zZ8L</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/2WfaOiMkCvy7...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/2Wfa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Livin' On A Prayer</td>\n",
       "      <td>249293</td>\n",
       "      <td>3</td>\n",
       "      <td>0.534</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.777</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.720</td>\n",
       "      <td>122.494</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>0J6mQxEZnlRt9ymzFntA6z</td>\n",
       "      <td>spotify:track:0J6mQxEZnlRt9ymzFntA6z</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/0J6mQxEZnlRt...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/0J6m...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If You Leave Me Now</td>\n",
       "      <td>235373</td>\n",
       "      <td>4</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.563</td>\n",
       "      <td>11</td>\n",
       "      <td>-6.784</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>0.275</td>\n",
       "      <td>104.183</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>0KMGxYKeUzK9wc5DZCt3HT</td>\n",
       "      <td>spotify:track:0KMGxYKeUzK9wc5DZCt3HT</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/0KMGxYKeUzK9...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/0KMG...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  duration_ms  track_number  danceability  \\\n",
       "0        Every Breath You Take       253920             7         0.820   \n",
       "1  Don't You (Forget About Me)       263040             1         0.660   \n",
       "2                   Take on Me       225280             1         0.573   \n",
       "3           Livin' On A Prayer       249293             3         0.534   \n",
       "4          If You Leave Me Now       235373             4         0.434   \n",
       "\n",
       "   energy  key  loudness  mode  speechiness  acousticness  instrumentalness  \\\n",
       "0   0.452    1    -9.796     1       0.0348        0.5430          0.002940   \n",
       "1   0.816    2    -6.610     1       0.0299        0.1680          0.018100   \n",
       "2   0.902    6    -7.638     0       0.0540        0.0180          0.001250   \n",
       "3   0.887    0    -3.777     1       0.0345        0.0768          0.000099   \n",
       "4   0.563   11    -6.784     1       0.0268        0.0197          0.000824   \n",
       "\n",
       "   liveness  valence    tempo            type                      id  \\\n",
       "0    0.0714    0.740  117.401  audio_features  1JSTJqkT5qHq8MDJnJbRE1   \n",
       "1    0.0608    0.678  111.346  audio_features  3fH4KjXFYMmljxrcGrbPj9   \n",
       "2    0.0928    0.876   84.412  audio_features  2WfaOiMkCvy7F5fcp2zZ8L   \n",
       "3    0.3250    0.720  122.494  audio_features  0J6mQxEZnlRt9ymzFntA6z   \n",
       "4    0.1280    0.275  104.183  audio_features  0KMGxYKeUzK9wc5DZCt3HT   \n",
       "\n",
       "                                    uri  \\\n",
       "0  spotify:track:1JSTJqkT5qHq8MDJnJbRE1   \n",
       "1  spotify:track:3fH4KjXFYMmljxrcGrbPj9   \n",
       "2  spotify:track:2WfaOiMkCvy7F5fcp2zZ8L   \n",
       "3  spotify:track:0J6mQxEZnlRt9ymzFntA6z   \n",
       "4  spotify:track:0KMGxYKeUzK9wc5DZCt3HT   \n",
       "\n",
       "                                          track_href  \\\n",
       "0  https://api.spotify.com/v1/tracks/1JSTJqkT5qHq...   \n",
       "1  https://api.spotify.com/v1/tracks/3fH4KjXFYMml...   \n",
       "2  https://api.spotify.com/v1/tracks/2WfaOiMkCvy7...   \n",
       "3  https://api.spotify.com/v1/tracks/0J6mQxEZnlRt...   \n",
       "4  https://api.spotify.com/v1/tracks/0KMGxYKeUzK9...   \n",
       "\n",
       "                                        analysis_url  time_signature  \n",
       "0  https://api.spotify.com/v1/audio-analysis/1JST...               4  \n",
       "1  https://api.spotify.com/v1/audio-analysis/3fH4...               4  \n",
       "2  https://api.spotify.com/v1/audio-analysis/2Wfa...               4  \n",
       "3  https://api.spotify.com/v1/audio-analysis/0J6m...               4  \n",
       "4  https://api.spotify.com/v1/audio-analysis/0KMG...               4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# playlist ID you would like to get track data from \n",
    "pl_id = 'spotify:playlist:1kthr7VG9a1oGC4bQixTas'\n",
    "\n",
    "# playlist_items() gets each item from the playlist\n",
    "response = sp.playlist_items(pl_id,\n",
    "                                limit=1, # maximum number of tracks to return\n",
    "                                offset=0, # the index of the first track to return\n",
    "                                fields='total', # which fields to return\n",
    "                                additional_types=['track']) # list of item types to return (we want tracks)\n",
    "total_num_tracks = response['total'] # total_num_tracks stores total number of tracks in playlist\n",
    "\n",
    "offset = 0 # will act as a pointer after every 100 tracks we go through\n",
    "limit = 100\n",
    "# print(total_num_tracks, int(total_num_tracks/limit))\n",
    "\n",
    "dfs = [] # data frames list\n",
    "for n in range(int(total_num_tracks/limit)+1): # we want to iterate 10 times bc we have 909 tracks in this cases (we will get data for every 100 tracks each loop)\n",
    "    response = sp.playlist_items(pl_id,\n",
    "                                limit=limit,\n",
    "                                offset=offset, \n",
    "                                fields='items.track.id,items.track.name,items.track.artist,items.track.track_number,items.track.duration_ms,items.track.id,total',\n",
    "                                additional_types=['track']) \n",
    "\n",
    "    # pprint(response)\n",
    "    track_ids = []\n",
    "    tracks = []    \n",
    "    for item in response['items']: # item is a dictionary that has dictionaries\n",
    "        track_ids.append(item['track']['id']) # in the dictionary \"item\", we are getting the value for key \"id\" from dictionary \"track\"\n",
    "        # {\"items\": {\"track\": {\"id\": '17CPezzLWzvGfpZW6X8XT0'}}}\n",
    "        # storing dictionary in track_info\n",
    "        track_info = {  \"name\":item['track']['name'],\n",
    "                        \"duration_ms\":item['track']['duration_ms'],\n",
    "                        \"track_number\":item['track']['track_number']    }\n",
    "        # {\"items\": {\"track\": {\"name\": 'Say You, Say Me'}}}\n",
    "        # {\"items\": {\"track\": {\"duration_ms\": 241066}}}\n",
    "        # {\"items\": {\"track\": {\"track_number\": 8}}}\n",
    "        tracks.append(track_info)\n",
    "\n",
    "    # get audio features from each track id from track_ids list\n",
    "    features = sp.audio_features(tracks=track_ids)\n",
    "\n",
    "    # print(len(tracks))\n",
    "\n",
    "    # for every rack, add/combine features list items to tracks list (both have dictionaries so we combine dictionaries)\n",
    "    for idx in range(len(tracks)):\n",
    "        tracks[idx].update(features[idx]) \n",
    "\n",
    "    df = pd.DataFrame(tracks) # create data frame df of tracks list\n",
    "    dfs.append(df) # add data frame to dfs list\n",
    "    # pprint(features)\n",
    "\n",
    "    if len(response['items']) == 0: # if no items, exit loop\n",
    "        break\n",
    "\n",
    "    offset = offset + len(response['items']) # increment offset by length of items in loop (move pointer every 100 items until it reaches end of tracks)\n",
    "    # print(offset, \"/\", response['total']) # display how many tracks we went through after each loop out of total number of tracks\n",
    "\n",
    "# Put all songs in 1 DataFrame\n",
    "df_combo = pd.concat(dfs) # combine the data frames in the dfs list to 1 data frame\n",
    "df_combo.head() # print first few rows of data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(914, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                0.0\n",
       "duration_ms         0.0\n",
       "track_number        0.0\n",
       "danceability        0.0\n",
       "energy              0.0\n",
       "key                 0.0\n",
       "loudness            0.0\n",
       "mode                0.0\n",
       "speechiness         0.0\n",
       "acousticness        0.0\n",
       "instrumentalness    0.0\n",
       "liveness            0.0\n",
       "valence             0.0\n",
       "tempo               0.0\n",
       "type                0.0\n",
       "id                  0.0\n",
       "uri                 0.0\n",
       "track_href          0.0\n",
       "analysis_url        0.0\n",
       "time_signature      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "# nothing is missing wooooooo\n",
    "df_combo.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo.to_csv ('spotify_playlist.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.**\n",
    "\n",
    "(**10 points**; Due Friday November 19th at 11:59pm; PDF) Pretend you're a company who is interested in the dataset you chose. Come up with at **least 7 questions** that you want to answer based on the variables (at the top of this document, **provide a short description of each of the variables in the model**).\n",
    "\n",
    "\n",
    "These questions can be about the relationships between variables, or how well one thing can predict another, clustering...etc, but note that in your final project you must use at least **1 supervised learning model** (includes both regression and classification models), **1 clustering model, and 1 instance of dimensionality reduction** (PCA or LASSO), so keep that in mind when creating questions. You can use more than one of these for a single question (e.g. using PCA and then doing linear regression on the components).\n",
    "\n",
    "You will be graded on the quality of the questions. Questions should be interesting and complex (e.g. questions like \"is this model more than 90% accurate?\" should be expanded to something like \"is this model accurate as measured by accuracy, examination of patterns in the confusion matrix and/or consistent accuracy across gender/race/income/education groups?\"). Questions related to the same model/analysis should be included as 1 question (for example, if you build a model predicting cat weight from cat height, cat age, and cat diet, the question should be somthing like \"which variables have the strongest impact on cat weight?\" instead of having three separate questions \"what is the impact of cat height on cat weight?\", \"what is the impact of cat age on cat weight?\", and \"\"what is the impact of cat diet on cat weight?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [What Makes a Song Likeable?](https://towardsdatascience.com/what-makes-a-song-likeable-dbfdb7abe404)\n",
    "\n",
    "- name: name of the song\n",
    "- duration_ms: The duration of the track in milliseconds\n",
    "- track_number: what number the song is in the playlist\n",
    "- danceability: describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity\n",
    "- energy: represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale\n",
    "- key: the estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on.\n",
    "- loudness: the overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks.\n",
    "- mode: indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n",
    "- speechiness: detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value.\n",
    "- acousticness: a confidence measure from 0.0 to 1.0 of whether the track is acoustic.\n",
    "- instrumentalness: predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”.\n",
    "- liveness: the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live.\n",
    "- valence: describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)\n",
    "- tempo: the overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece, and derives directly from the average beat duration.\n",
    "- type: type of attribute we are looking at (audio features)\n",
    "- id: track id\n",
    "- uri: a unique identifier (link) of a song, album or playlist found in the Share menu\n",
    "- track_href: this attribute specifies the URL of the page the link goes to\n",
    "- analysis_url: playlist information in json format\n",
    "- time_signature: An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our compnay wants to use the Spotify API in order to promote songs to its customers based on what kind of music they like to listen to. To do this, we are using the assumptions of `valence`, `energy`, `tempo`, `mode` provided by Spotify to classify each song as Happy, Angry, Calm or Sad. \n",
    "\n",
    "### Questions:\n",
    "1. Journalist Miriam Quick performed an investigation to identify the saddest and the happiest songs ever listed on the top of the Billboard singles chart. To conduct this investigaiton, Quick used numerical parameters of valence and energy, provided by the Spotify Web API. Spotify's internal algorithm measures valence from 0 to 1 based on positive or negative moods, while energy, whcih is also measured from 0 to 1, indicates how lively and dynamic a song is. As we can see in the scatterplot below, Quick splits the graph into 4 quadrants of mood (Happy, Angry, Calm and Sad) based on the valence and energy of each song. Based on this investigation, would **Gaussian Mixture Models (EM)** be able to accurately cluster the data into the 4 quadrants of mood given the `valence` and `energy`? If not, what other assumptions can we make of the clusters (meaning what characterizes each cluster)?\n",
    "\n",
    "<img src=\"canzoni_distr.jpeg\">\n",
    "\n",
    "2. Tempo is the the overall estimated tempo of a track in beats per minute (BPM). Higher-enrgetic songs tend to be faster in tempo because they are typically played in major key (meaning when mode is 1), whereas slower-paced songs tend to be slower in tempo because they are typically played in minor key (meaning when mode is 0). Would adding `tempo` to the **Gaussian Mixture Models (EM)** clustering model improve the fit of the model? What other assumptions can we make of the clusters? Use a sillhouette score to assess the performance of your model.\n",
    "\n",
    "3. What is the relationship between `valence` and `energy`? Is the relationship between those two variables different for the `mode` of each track? How can you tell? What can we infer about the relationship between `valence` and `energy` of a song given its `mode`? Meaning, what can we say about a track's 4 quadrants of mood (Happy, Angry, Calm and Sad) based on the relationship between `valence` and `energy` of a song given its `mode`?\n",
    "\n",
    "4. Using **KNN**, **Decision Tree**, AND **Logistic Regression Model**, predict the `mode` (major or minor) of each track. Using accuracy and confusion matrices, which model did best and how can you tell? What does this infer about how each model classifies each track?\n",
    "\n",
    "5. Using the classification or regression model with the best performance found in the previous question, is the model more accurate for `tempos` with values less that 110, between 110 and 160, or greater than 160? What are the potential accuracy implications if this model were more accurate for different `tempos`.\n",
    "\n",
    "6. With the **Logistic Regression Model** you made in question #3, record the MSE/R2 for both training/test sets. Discuss the performance of the model. Build a NEW **Logistic Regression Model**, but using **PCA**. Fit your model using the components you found using a scree plot and record the MSE/R2 for both training/test sets. Discuss how the performance of the model built using **PCA** differs from the model built just using **Logistic Regression Model**.\n",
    "\n",
    "7. Looking at the scree plot, how much of the information was retained in the model with using how many components? Looking at the relationship between the priciple components, are there variables that have less of an impact overall on the data? Meaning, could we retain most of the information from the original data with just a few variables? What does this mean in terms of relationships between each of the audio features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.**\n",
    "\n",
    "(**27 points**; Due Monday November 29th RIGHT BEFORE CLASS (if you do not have class on Monday it is due at 11:59pm); PDF) Now put on your data scientist hat. Write an **ORGANIZED analysis plan** to answer **3** of the questions you came up with. Think about which of the questions need a predictive model, which need a clustering model, which need dimensionality reduction, and which maybe need just visualizations/summaries. (at the top of this document, provide a short description of each of the variables in the model)\n",
    "\n",
    "*YOU MUST USE at least 1 supervised learning model, 1 clustering model and 1 instance of dimensionality reduction (two or more of these could be used to answer the same question)*.\n",
    "\n",
    "Write up this plan as if you're submitting it to a company to tell them what you're planning to do. CLEARLY mark where each part (a-c) is and answer each part separately. For **each** question you need to:\n",
    "\n",
    "<ul>\n",
    "    <li> a) describe the analysis you're planning (include details like whether you're using standardization, regularization, model validation, distance/similarity metrics, how you'll choose clusters or hyperparameters, which variables you're using...etc)\n",
    "    <li> b) explain <b>why</b> this analysis and the choices you described above are good and explicitly <b>how</b> these methods will answer the question.\n",
    "    <li> c) describe <b>two</b> ggplot data visualizations you'll use to support your answers (graphs must be in ggplot, the ONLY acception is a dendrogram for HAC). \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Journalist Miriam Quick performed an investigation to identify the saddest and the happiest songs ever listed on the top of the Billboard singles chart. To conduct this investigaiton, Quick used numerical parameters of valence and energy, provided by the Spotify Web API. Spotify's internal algorithm measures valence from 0 to 1 based on positive or negative moods, while energy, which is also measured from 0 to 1, indicates how lively and dynamic a song is. As we can see in the scatterplot below, Quick splits the graph into 4 quadrants of mood (Happy, Angry, Calm and Sad) based on the valence and energy of each song. Based on this investigation, would **Gaussian Mixture Models (EM)** be able to accurately cluster the data into the 4 quadrants of mood given the `valence` and `energy`? If not, what other assumptions can we make of the clusters (meaning what characterizes each cluster)?\n",
    "\n",
    "    a) describe the analysis you're planning (include details like whether you're using standardization, regularization, model validation, distance/similarity metrics, how you'll choose clusters or hyperparameters, which variables you're using...etc)\n",
    "    - I will be using the **Gaussian Mixture Models (EM)** clustering algorithm because it will use soft assignments for each of the data points, meaning that it assigns each data point a probability of being assigned to each cluster.\n",
    "    - I will choose 4 components as the hyperparameters because in order to see if the algorithm follows the same trend of 4 quadrants of mood (Happy, Angry, Calm and Sad) as Quick's analysis, I would like to see how the algorithm would categorize the data given having to make 4 clusters.\n",
    "\n",
    "    b) explain <b>why</b> this analysis and the choices you described above are good and explicitly <b>how</b> these methods will answer the question.\n",
    "    - In this case, this would be beneficial because since **Gaussian Mixture Models (with EM)** assumes that the clusters are elliptical, the clustering algorithms will be able to cluster specific categories with more felxibility.\n",
    "    - The clusters that have more of a spread will use probabilistic assignments that would help assign the data points in the clusters it is proabable to be in.\n",
    "    - Using **Gaussian Mixture Model's** elliptical clusters will give us an idea if the probability of the data points that are assigned to each cluster follow Miriam Quick's 4 quadrants of mood trend.\n",
    "\n",
    "    c) describe <b>two</b> ggplot data visualizations you'll use to support your answers (graphs must be in ggplot, the ONLY acception is a dendrogram for HAC). \n",
    "    - I will create a scatterplot to check the linearity assumption for `valence` and `energy` to see what the relationship is between the two variables.\n",
    "    - I will create another scatterplot to plot all of the tracks and then use the 4 components from the clustering model to categorize which data points fall into which cluster.\n",
    "\n",
    "\n",
    "2. What is the relationship between `valence` and `energy`? Is the relationship between those two variables different for the `mode` of each track? How can you tell? What can we infer about the relationship between `valence` and `energy` of a song given its `mode`? Meaning, what can we say about a track's 4 quadrants of mood (Happy, Angry, Calm and Sad) based on the relationship between `valence` and `energy` of a song given its `mode`?\n",
    "\n",
    "    a) describe the analysis you're planning (include details like whether you're using standardization, regularization, model validation, distance/similarity metrics, how you'll choose clusters or hyperparameters, which variables you're using...etc)\n",
    "    - We will be using `valence`, `energy` and `mode` to create a scatterplot to see the realtionship between all 3 variables.\n",
    "\n",
    "    b) explain <b>why</b> this analysis and the choices you described above are good and explicitly <b>how</b> these methods will answer the question.\n",
    "    - Referring to the first question, I would like to see if `mode` has any relationship with `valence` and `energy` given the clusters that were made by the **Gaussian Mixture Models (with EM)**.\n",
    "    - If it does, then I am curious to see what this says about the mode of the songs in each given cluster and how it would change the interpretation of each cluster.\n",
    "\n",
    "    c) describe <b>two</b> ggplot data visualizations you'll use to support your answers (graphs must be in ggplot, the ONLY acception is a dendrogram for HAC). \n",
    "    - I will first create a scatterplot to see the relationship between `valence` and `energy`.\n",
    "    - I will create another scatterplot factoring or color coding the 2 modes to see if there is a relationship between between `valence` and `energy` of a song given its `mode`.\n",
    "\n",
    "\n",
    "3. Using **Logistic Regression Model**, predict the `mode` (major or minor) of each track. With **Logistic Regression Model**, record the MSE/R2 for both training/test sets. Discuss the performance of the model. Build a NEW **Logistic Regression Model**, but using **PCA**. Fit your model using the components you found using a scree plot and record the MSE/R2 for both training/test sets. Discuss how the performance of the model built using **PCA** differs from the model built just using **Logistic Regression Model**.\n",
    "\n",
    "    a) describe the analysis you're planning (include details like whether you're using standardization, regularization, model validation, distance/similarity metrics, how you'll choose clusters or hyperparameters, which variables you're using...etc)\n",
    "    - I will use a 10 fold cross validation for my model validation. Then I will store both the train and test accuracies (MSE/R2) to check for overfitting.\n",
    "    - I will also print out a confusion matrix to tell us how well the model is performing.\n",
    "    - Then I will do the same steps when building another **Logistic Regression Model**, but using **PCA**.\n",
    "    - Using a scree plot, this will tell us the number of factors (x-axis) and eigenvalues (y-axis). In this model, the x-axis tells us the number of principal components to use.\n",
    "    - I will then record the MSE/R2 for both training/test sets for the new model.\n",
    "    - This will help us compare the **Logistic Regression Model** without using **PCA** and **Logistic Regression Model**  using **PCA**.\n",
    "\n",
    "    b) explain <b>why</b> this analysis and the choices you described above are good and explicitly <b>how</b> these methods will answer the question.\n",
    "    - **PCA** is a way of rotating the axes of the data to take advantage of the relationships between different variables and create a new set of axes that is very efficient at describing the variation in the data. It is efficient because we are only retaining a handful of our principal components and still covering almost all the information from the original data. \n",
    "    - Therefore, looking at the scree plot, we can identify how much of the information was retained in the model while using fewer components/variables. \n",
    "    - This can tell us the relationships between each of the audio features.\n",
    "    - It might be better for us to use less audio features that are not necessary to the relationship between other audio features. Meaning, it might get rid of unnecessary variables that do not have a significant relationship to the other variables. \n",
    "    - Instead of having so many audio features with a slight relationship, can the new sets of axes describe the variation better than before?\n",
    "\n",
    "    c) describe <b>two</b> ggplot data visualizations you'll use to support your answers (graphs must be in ggplot, the ONLY acception is a dendrogram for HAC). \n",
    "    - The explained variance will grab for each of the different components how much variance that specific component accounts for. We can use the elbow method on the explained variance scree plot to to look for the point of inflection on the graph. The point of inflection can tell us up until what components we can keep.\n",
    "    - Since priciniple components analysis orders the priciniple components from most to least variability explained, cumulative variance will tell us how much variance does the first few components account for total. Graphing a cumulative variance scree plot can tell us what the first component is where the cumulative variance exceeds our threshhold (of maybe 90%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.**\n",
    "(**5 points**; Due Friday December 3rd at 11:59pm) Peer review + write a critique of another person/group's plan (~ 1 page). You should answer:\n",
    "\n",
    "<ul>\n",
    "    <li> what does this plan do well?\n",
    "    <li> what could be improved (give specifics) and why?\n",
    "    <li> what are some (perhaps unavoidable) limitations of the data/analysis plan?\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II\n",
    "\n",
    "### **5.**\n",
    "(**40 points**; Due Monday December 13th at 11:59pm; PDF of Jupyter Notebook) **Perform the planned analyses** (be sure to note in markdown if there were any changes to your analysis plan since part #3 and **why**), and **make the graphs in a python notebook**. You should also include written (in markdown) answers to each of the 3 questions that you asked:\n",
    "\n",
    "<ul>\n",
    "    <li> a) <b>the analysis code.</b>\n",
    "    <li> b) <b>explicit answer to the question with detailed responses of how you came to this answer and the answer's importance.</b> This should be targeted at an audience that are NOT familiar with Data Science (e.g. pretend you're presenting these results to shareholders/your boss).\n",
    "    <li> c) <b>two ggplot data visualizations + captions</b> (graphs will be graded on how efficient and clear they are, so make sure you make good aesthetic choices that help emphasize your message).\n",
    "</ul>\n",
    "\n",
    "\n",
    "Answers should be clear, concise, and complete. You will be graded on your code, the clarity of your responses, and the correctness of your methods. Save that notebook as a PDF. You must clearly label each question and the analyses that apply to it using Markdown.Don't forget to turn in a README with this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.**\n",
    "\n",
    "(**8 points**; Due Sunday December 12th, 11:59pm; Video link) **Prepare a short presentation of the results** (powerpoint, prezi, keynote...etc, DO NOT just scroll through your notebook.). Make a short (3-5 minute, not under or over) **video presentation** explaining what you found. Upload it to youtube or similar site (you can put your video as Unlisted if you don't want anyone else to see)\n",
    "\n",
    "\n",
    "I recommend OBS Streamlabs if you want to record your screen (with the presentation/data) and yourself presenting at one time. Or get someone to film you presenting it on a screen (or if needed print our your slides and hold them up!). Or if you're on a Mac you can use QuickTime to record your screen while you present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7.**\n",
    "\n",
    "(**7 points**; During your scheduled Final) Watch 2 other students' videos and answer the following questions on Canvas:\n",
    "<ul>\n",
    "    <li> What are 2 things you enjoyed about their presentation?\n",
    "    <li> What is 1 thing they could be more clear about when presenting?\n",
    "    <li> What was the key idea you took away from their presentation?\n",
    "</ul> \n",
    "\n",
    "and fill out the feedback form given to you by Chelsea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist\n",
    "To review, throughout the project you'll need to submit:\n",
    "\n",
    "1. the **name/a link to the data set** you're planning to use (you don't have to do this, but it saves you time/effort, in case your dataset won't work out)\n",
    "2. A **PDF or text submission** with your questions.\n",
    "3. A **PDF** with your analysis plan.\n",
    "4. A **text submission or PDF** on Canvas with your critique of *another* students plan.\n",
    "5. A **PDF** of your python notebooks. Please get rid of extra analyses/code that you did not end up using. You must clearly indicate where each question is being answered. Also include a **README**.\n",
    "6. A **link** to a short video presentation (do not send the video directly). \n",
    "7. A **text submission or PDF** on Canvas with your peer video feedback."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "492c417cf4bd3d8ef83c2c7271d2b849cb89ee688160599385917934fff29a93"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
